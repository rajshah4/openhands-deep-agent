{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parallel Sub-Agent Research\n",
        "\n",
        "This notebook demonstrates **parallel sub-agent delegation** - spawning multiple agents to research sub-questions simultaneously.\n",
        "\n",
        "| Pattern | Notebook 01 | This Notebook |\n",
        "|---------|:-----------:|:-------------:|\n",
        "| Sequential searches | ‚úÖ | ‚ùå |\n",
        "| Parallel sub-agents | ‚ùå | ‚úÖ |\n",
        "| Isolated context per question | ‚ùå | ‚úÖ |\n",
        "| Speed | Slower | **Faster** |\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                    ‚îÇ   Planner   ‚îÇ\n",
        "                    ‚îÇ   Agent     ‚îÇ\n",
        "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                           ‚îÇ Creates sub-questions\n",
        "           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "           ‚ñº               ‚ñº               ‚ñº\n",
        "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "    ‚îÇ Sub-Agent  ‚îÇ  ‚îÇ Sub-Agent  ‚îÇ  ‚îÇ Sub-Agent  ‚îÇ\n",
        "    ‚îÇ     #1     ‚îÇ  ‚îÇ     #2     ‚îÇ  ‚îÇ     #3     ‚îÇ\n",
        "    ‚îÇ (Question) ‚îÇ  ‚îÇ (Question) ‚îÇ  ‚îÇ (Question) ‚îÇ\n",
        "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "          ‚îÇ               ‚îÇ               ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                          ‚ñº\n",
        "                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                   ‚îÇ Synthesizer ‚îÇ\n",
        "                   ‚îÇ   Agent     ‚îÇ\n",
        "                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install openhands-sdk litellm python-dotenv tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from typing import List\n",
        "from pydantic import Field\n",
        "from tavily import TavilyClient\n",
        "from openhands.sdk import LLM, Agent, Conversation, Tool, Action, Observation, ToolDefinition, TextContent\n",
        "from openhands.sdk.tool import register_tool, ToolExecutor\n",
        "from openhands.tools.file_editor import FileEditorTool\n",
        "\n",
        "print(f\"Model: {os.getenv('LLM_MODEL', 'openai/gpt-4o')}\")\n",
        "\n",
        "# Observability status\n",
        "if os.getenv(\"LMNR_PROJECT_API_KEY\"):\n",
        "    print(\"‚úì Observability: Laminar tracing enabled\")\n",
        "else:\n",
        "    print(\"‚Ñπ Observability: Set LMNR_PROJECT_API_KEY for tracing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create LLM and Tavily Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LLMs\n",
        "llm = LLM(\n",
        "    model=\"openai/gpt-4o\",\n",
        "    api_key=os.getenv(\"LLM_API_KEY\"),\n",
        "    base_url=os.getenv(\"LLM_BASE_URL\", None),\n",
        ")\n",
        "\n",
        "synthesis_llm = LLM(\n",
        "    model=\"openai/gpt-5.1\",\n",
        "    api_key=os.getenv(\"LLM_API_KEY\"),\n",
        "    base_url=os.getenv(\"LLM_BASE_URL\", None),\n",
        ")\n",
        "\n",
        "# Tavily client and tool\n",
        "tavily = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
        "\n",
        "class SearchAction(Action):\n",
        "    query: str = Field(description=\"Search query\")\n",
        "\n",
        "class SearchObservation(Observation):\n",
        "    results: str = Field(description=\"Results\")\n",
        "    @property\n",
        "    def to_llm_content(self): return [TextContent(text=self.results)]\n",
        "\n",
        "class SearchExecutor(ToolExecutor):\n",
        "    def __call__(self, action: SearchAction, conversation=None) -> SearchObservation:\n",
        "        try:\n",
        "            r = tavily.search(query=action.query, max_results=5)\n",
        "            text = \"\\n\\n\".join([\n",
        "                f\"**{x['title']}**\\n{x['content'][:300]}\\nSource: {x['url']}\"\n",
        "                for x in r['results']\n",
        "            ])\n",
        "            return SearchObservation(results=text or \"No results found\")\n",
        "        except Exception as e:\n",
        "            return SearchObservation(results=f\"Search failed: {str(e)}\")\n",
        "\n",
        "class SearchTool(ToolDefinition[SearchAction, SearchObservation]):\n",
        "    @classmethod\n",
        "    def create(cls, conv_state) -> List[\"SearchTool\"]:\n",
        "        return [cls(description=\"Web search via Tavily\",\n",
        "                    action_type=SearchAction, observation_type=SearchObservation, \n",
        "                    executor=SearchExecutor())]\n",
        "\n",
        "register_tool(\"TavilySearch\", SearchTool.create)\n",
        "print(\"‚úì LLMs and Tavily tool ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define Sub-Agent Research Function\n",
        "\n",
        "Each sub-agent researches ONE sub-question independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "\n",
        "def research_sub_question(question: str, index: int) -> dict:\n",
        "    \"\"\"\n",
        "    Spawn a sub-agent to research a single sub-question.\n",
        "    Each agent writes its findings to a unique file.\n",
        "    \"\"\"\n",
        "    print(f\"  üîç Sub-agent {index+1} starting: {question[:50]}...\")\n",
        "    \n",
        "    # Create a dedicated agent for this sub-question\n",
        "    sub_agent = Agent(\n",
        "        llm=llm,\n",
        "        tools=[Tool(name=\"TavilySearch\"), Tool(name=FileEditorTool.name)],\n",
        "    )\n",
        "    \n",
        "    # Create isolated conversation for this sub-agent\n",
        "    sub_conversation = Conversation(agent=sub_agent, workspace=cwd)\n",
        "    \n",
        "    # Each sub-agent writes to its own file\n",
        "    output_file = f\"findings_{index+1}.md\"\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    Research this specific question: {question}\n",
        "    \n",
        "    1. Use TavilySearch to find 2-3 relevant sources\n",
        "    2. Write your findings to the file `{output_file}` including:\n",
        "       - Key points discovered\n",
        "       - Source URLs for citations\n",
        "    \n",
        "    Be concise but thorough.\n",
        "    \"\"\"\n",
        "    \n",
        "    sub_conversation.send_message(prompt)\n",
        "    sub_conversation.run()\n",
        "    \n",
        "    # Read findings from the file the agent created\n",
        "    findings = \"\"\n",
        "    try:\n",
        "        with open(output_file, \"r\") as f:\n",
        "            findings = f.read()\n",
        "    except FileNotFoundError:\n",
        "        findings = f\"[Sub-agent {index+1} did not write findings]\"\n",
        "    \n",
        "    print(f\"  ‚úì Sub-agent {index+1} complete\")\n",
        "    \n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"findings\": findings,\n",
        "        \"index\": index\n",
        "    }\n",
        "\n",
        "print(\"‚úì Sub-agent research function ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run Parallel Research\n",
        "\n",
        "Uses ThreadPoolExecutor to run sub-agents in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Research topic and sub-questions\n",
        "topic = \"Latest breakthroughs in AI agents and autonomous systems (2024-2025)\"\n",
        "\n",
        "sub_questions = [\n",
        "    \"What are the major advances in LLM-based autonomous agents in 2024-2025?\",\n",
        "    \"How has multi-agent collaboration evolved in AI systems?\",\n",
        "    \"What new tools and frameworks have emerged for building AI agents?\",\n",
        "    \"What are the key challenges and limitations of current AI agents?\",\n",
        "    \"What real-world applications are using AI agents successfully?\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"PARALLEL RESEARCH: {topic}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nResearching {len(sub_questions)} sub-questions in parallel...\\n\")\n",
        "\n",
        "# Run sub-agents in parallel using ThreadPoolExecutor\n",
        "start_time = time.time()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=len(sub_questions)) as executor:\n",
        "    # Submit all sub-agent tasks\n",
        "    futures = [\n",
        "        executor.submit(research_sub_question, q, i) \n",
        "        for i, q in enumerate(sub_questions)\n",
        "    ]\n",
        "    \n",
        "    # Collect results as they complete\n",
        "    results = [f.result() for f in futures]\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n‚úì All {len(sub_questions)} sub-agents complete in {elapsed:.1f}s\")\n",
        "\n",
        "# Sort by original index\n",
        "results.sort(key=lambda x: x['index'])\n",
        "\n",
        "# Save raw findings to file\n",
        "findings_text = f\"# Research Findings: {topic}\\n\\n\"\n",
        "for r in results:\n",
        "    findings_text += f\"## {r['index']+1}. {r['question']}\\n\\n\"\n",
        "    findings_text += f\"{r['findings']}\\n\\n---\\n\\n\"\n",
        "\n",
        "with open(\"parallel_findings.md\", \"w\") as f:\n",
        "    f.write(findings_text)\n",
        "\n",
        "print(\"‚úì Raw findings saved to parallel_findings.md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Synthesize with GPT-5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"SYNTHESIS: Creating comprehensive report (GPT-5.1)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create synthesis agent\n",
        "synthesis_agent = Agent(\n",
        "    llm=synthesis_llm,\n",
        "    tools=[Tool(name=FileEditorTool.name)],\n",
        ")\n",
        "\n",
        "synthesis_conversation = Conversation(agent=synthesis_agent, workspace=cwd)\n",
        "\n",
        "SYNTHESIS_PROMPT = \"\"\"\n",
        "Read `parallel_findings.md` which contains research findings from multiple parallel searches.\n",
        "\n",
        "Synthesize these into a comprehensive report and write it to `parallel_report.md`:\n",
        "\n",
        "# [Topic Title]\n",
        "\n",
        "## Executive Summary\n",
        "(2-3 paragraph overview)\n",
        "\n",
        "## Key Findings\n",
        "(Organize by theme, not by original question order)\n",
        "(Cite sources inline)\n",
        "\n",
        "## Analysis & Implications\n",
        "(What do these findings mean?)\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "## References\n",
        "(All sources with URLs)\n",
        "\n",
        "Write professionally. Synthesize and connect ideas across the different sub-questions.\n",
        "\"\"\"\n",
        "\n",
        "synthesis_conversation.send_message(SYNTHESIS_PROMPT)\n",
        "synthesis_conversation.run()\n",
        "\n",
        "print(\"\\n‚úì Synthesis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: View the Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "try:\n",
        "    with open(\"parallel_report.md\", \"r\") as f:\n",
        "        display(Markdown(f.read()))\n",
        "except FileNotFoundError:\n",
        "    print(\"parallel_report.md not found - run the cells above first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Sequential vs Parallel\n",
        "\n",
        "| Approach | Notebook 01 | This Notebook |\n",
        "|----------|-------------|---------------|\n",
        "| **Execution** | Sequential | Parallel (ThreadPoolExecutor) |\n",
        "| **Sub-agents** | 1 agent, multiple turns | N agents, isolated context |\n",
        "| **Speed** | ~N √ó search_time | ~1 √ó search_time |\n",
        "| **Context** | Shared (can grow large) | Isolated (stays small) |\n",
        "| **Error handling** | One failure stops all | Failures isolated |\n",
        "\n",
        "**When to use parallel:**\n",
        "- Many independent sub-questions\n",
        "- Speed is important\n",
        "- Sub-questions don't depend on each other\n",
        "\n",
        "**When to use sequential (Notebook 01):**\n",
        "- Sub-questions build on each other\n",
        "- Need to refine queries based on earlier results\n",
        "- Simpler debugging"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
